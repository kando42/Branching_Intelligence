# Ground-Up Implementations of Decision Tree Algorithms

## Introduction
In this project, I implement several decision tree algorithms from scratch, including:
- **Decision Tree**
- **Random Forest**
- **AdaBoost**

The goal is to understand the inner workings of these algorithms and compare my implementations against popular libraries such as scikit-learn.

## Overview of Algorithms
### Decision Tree
- Basic idea: Recursive partitioning of the dataset.
- Key metrics: Information gain, Gini impurity.

### Random Forest
- Ensemble of decision trees.
- Aggregates the predictions of multiple trees for improved accuracy.

### AdaBoost
- Boosting algorithm that combines weak learners (decision trees) into a strong classifier.
- Adjusts weights on training samples based on previous errors.

